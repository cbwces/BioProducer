{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import multiprocessing\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import conll2000\n",
    "from nltk.chunk import tree2conlltags\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchcrf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_encoder(raw_list, max_pad=99999):\n",
    "    # 将字符进行编码， raw_list:经过标记的语料\n",
    "    max_sent_lenth = 0\n",
    "    all_tag_list = []\n",
    "    for raw_in_sent in raw_list:\n",
    "        all_tag_list.extend(raw_in_sent)\n",
    "        if len(raw_in_sent) > max_sent_lenth:\n",
    "            max_sent_lenth = len(raw_in_sent)\n",
    "    \n",
    "    if max_pad > max_sent_lenth:\n",
    "        max_pad = max_sent_lenth\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(all_tag_list)\n",
    "    encoded_list = np.zeros((len(raw_list), max_sent_lenth))\n",
    "    for pos, raw_in_sent in enumerate(raw_list):\n",
    "        encoded_list[pos] = text_padding(label_encoder.transform(raw_in_sent)+1, max_pad)\n",
    "        \n",
    "    return encoded_list, label_encoder\n",
    "\n",
    "def text_padding(real_value_sequence, max_pad):\n",
    "    # 使所有句子的长度相同（不足的补0）\n",
    "    if len(real_value_sequence) <= max_pad:\n",
    "        remain_space = max_pad - len(real_value_sequence)\n",
    "        return np.concatenate((real_value_sequence, np.zeros(remain_space)))\n",
    "    \n",
    "    else:\n",
    "        return real_value_sequence[:max_pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConllDataset(Dataset):\n",
    "    # 构建输入神经网路的数据集\n",
    "    def __init__(self, part_speech, iob):\n",
    "        self.part_speech = part_speech\n",
    "        self.iob = iob\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.part_speech)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.part_speech[idx], self.iob[idx])\n",
    "    \n",
    "class TagNet(nn.Module):\n",
    "    # LSTM+CRF模型构造\n",
    "    def __init__(self, n_words, n_tags, embedding_dim=50, lstm_dim=32):\n",
    "        #初始化构造\n",
    "        super(TagNet, self).__init__()\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(n_words, embedding_dim=embedding_dim)\n",
    "        self.lstm_layer = nn.LSTM(embedding_dim, lstm_dim, num_layers=2, \n",
    "                                  dropout=0.2, bidirectional=True, batch_first=True)\n",
    "        self.linear_layer = nn.Linear(2*lstm_dim, n_tags)\n",
    "        self.crf_layer = CRF(n_tags, batch_first=True)\n",
    "        \n",
    "    def forward(self, x, y, mask, decode=False):\n",
    "        #前向传播\n",
    "        x = self.embedding_layer(x)\n",
    "        x, state = self.lstm_layer(x)\n",
    "        x = F.relu(x)\n",
    "        output = self.linear_layer(x)\n",
    "        if decode == False:\n",
    "            output = self.crf_layer(output, y, mask=mask, reduction='mean') #带负号的损失函数\n",
    "            return -output\n",
    "        else:\n",
    "            return self.crf_decoder(output, mask)\n",
    "        \n",
    "    def crf_decoder(self, x, mask):\n",
    "        #标签解码\n",
    "        return self.crf_layer.decode(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取coll2000语料的词性和IOB标记，逐句存放\n",
    "train_part_speech_list = []\n",
    "train_iob_list = []\n",
    "for tree in conll2000.chunked_sents():\n",
    "    train_part_speech_in_sent = []\n",
    "    train_iob_in_sent = []\n",
    "    tags = tree2conlltags(tree)\n",
    "    for tg in tags:\n",
    "        train_part_speech_in_sent.append(tg[1])\n",
    "        train_iob_in_sent.append(tg[2])\n",
    "    train_part_speech_list.append(train_part_speech_in_sent)\n",
    "    train_iob_list.append(train_iob_in_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分别对词性和IOB进行编码\n",
    "encoded_part_speech, part_speech_encoder = str_encoder(train_part_speech_list)\n",
    "encoded_iob, iob_encoder = str_encoder(train_iob_list)\n",
    "\n",
    "# 完成数据输入准备\n",
    "conll_corpus_dataset = ConllDataset(encoded_part_speech, encoded_iob)\n",
    "conll_loader = DataLoader(conll_corpus_dataset, batch_size=64, shuffle=True, num_workers=multiprocessing.cpu_count())\n",
    "\n",
    "# 统计词性和iob的种类个数\n",
    "sample_num = encoded_part_speech.shape[0]\n",
    "n_words = np.unique(encoded_part_speech).shape[0]\n",
    "n_tags = np.unique(encoded_iob).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172it [00:18,  9.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 0: 25.18171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "172it [00:18,  9.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 1: 7.06643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "172it [00:19,  8.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 2: 5.48121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "172it [00:23,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 3: 4.93338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 网络结构、优化器、损失函数初始化\n",
    "net = TagNet(n_words, n_tags).to('cuda')\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "# 训练（使用GPU）\n",
    "EPOCHS = 4\n",
    "for epk in range(EPOCHS):\n",
    "    mean_crf_loss = 0\n",
    "    \n",
    "    for pos, (x, y) in tqdm(enumerate(conll_loader)):\n",
    "        x = x.long().to('cuda')\n",
    "        y = y.long().to('cuda')\n",
    "        mask = torch.where(y > 0, y, torch.zeros(1).long().to('cuda')).bool()\n",
    "        \n",
    "        optimizer.zero_grad() #优化器梯度清零\n",
    "        output = net(x, y, mask) #前向传播\n",
    "        output.backward() #反向传播\n",
    "        optimizer.step() #更新参数\n",
    "        \n",
    "        mean_crf_loss += output\n",
    "    \n",
    "    mean_crf_loss /= (pos + 1)\n",
    "    print(\"Loss on epoch %d: %.5f\" % (epk, mean_crf_loss))\n",
    "        \n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "torch.save(net.state_dict(), \"../models/lstmcrf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TagNet(\n",
       "  (embedding_layer): Embedding(45, 50)\n",
       "  (lstm_layer): LSTM(50, 32, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (linear_layer): Linear(in_features=64, out_features=8, bias=True)\n",
       "  (crf_layer): CRF(num_tags=8)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取模型\n",
    "model = torch.load(\"../models/lstmcrf\")\n",
    "net = TagNet(n_words, n_tags)\n",
    "net.load_state_dict(model)\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikidata_dataset(dic_datas, encoder):\n",
    "    # 对维基数据进行编码\n",
    "    data_dict = defaultdict(list)\n",
    "    for key, value in tqdm(dic_datas.items()):\n",
    "        for sentence in value:\n",
    "            sent_raw_container = []\n",
    "            sent_tag_container = []\n",
    "            for word_tag_pair in sentence[1]:\n",
    "                sent_raw_container.append(word_tag_pair[0])\n",
    "                sent_tag_container.append(word_tag_pair[1])\n",
    "            try:\n",
    "                data_dict[key].append((sent_raw_container, sent_tag_container, encoder.transform(sent_tag_container) + 1))\n",
    "            except ValueError:\n",
    "                continue\n",
    "            \n",
    "    return data_dict\n",
    "\n",
    "def wiki_tagger(token_content, max_len=78):\n",
    "    # 对数据进行标注\n",
    "    test = np.zeros((len(token_content), 78))\n",
    "    \n",
    "    for pos, sentence in enumerate(token_content):\n",
    "        test[pos] = text_padding(sentence, max_len)\n",
    "        \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "# 读取词标注文本\n",
    "with open(\"../datas/token/animal_text_word_token.pkl\", 'rb') as f:\n",
    "    animal_tokens = pickle.load(f)\n",
    "with open(\"../datas/token/plant_text_word_token.pkl\", 'rb') as f:\n",
    "    plant_tokens = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2419/2419 [00:21<00:00, 114.77it/s]\n",
      "100%|██████████| 14510/14510 [03:00<00:00, 80.21it/s] \n"
     ]
    }
   ],
   "source": [
    "animal_tokens_dict = wikidata_dataset(animal_tokens, part_speech_encoder)\n",
    "plant_tokens_dict = wikidata_dataset(plant_tokens, part_speech_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2398/2398 [03:14<00:00, 12.31it/s]\n",
      "100%|██████████| 14497/14497 [27:54<00:00,  8.65it/s] \n"
     ]
    }
   ],
   "source": [
    "species_collector = defaultdict(list) # 收集结果\n",
    "\n",
    "# 预测标注值\n",
    "net = net.to('cuda')\n",
    "with torch.no_grad():\n",
    "    for text_dict in [animal_tokens_dict, plant_tokens_dict]:\n",
    "        for key in tqdm(text_dict.keys()):\n",
    "            text = []\n",
    "            tag_label_encoded = []\n",
    "            for t in text_dict[key]:\n",
    "                text.append(t[0])\n",
    "                tag_label_encoded.append(t[2])\n",
    "            in_tensor = torch.from_numpy(wiki_tagger(tag_label_encoded)).long().to('cuda')\n",
    "            mask = torch.where(in_tensor > 0, in_tensor, torch.zeros(1).long().to('cuda')).bool()\n",
    "            output = net(in_tensor, None, mask, decode=True) \n",
    "            \n",
    "            # 标签解码，并保存为“(词性，iob标注)”的形式\n",
    "            for t, iob in zip(text, output):\n",
    "                species_collector[key].append((t, iob_encoder.inverse_transform(np.asarray(iob) - 1)))\n",
    "                \n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del animal_tokens, plant_tokens, animal_tokens_dict, plant_tokens_dict\n",
    "gc.collect()\n",
    "with open('../datas/tuple/data_tagger_after_lstmcrf.pkl', 'wb') as bfile:\n",
    "    pickle.dump(species_collector, bfile, protocol=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
