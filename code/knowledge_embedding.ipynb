{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import random\n",
    "import pickle\n",
    "import multiprocessing\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_bag_processer(tuple_dict): # 只挑选所有元素仅包含唯一单词的三元组\n",
    "    \n",
    "    newer_dict = defaultdict(list)\n",
    "    total_word_set = set()\n",
    "    for key, values in tuple_dict.items():\n",
    "        for word_lists, full_sent in values:\n",
    "            # 过滤词组\n",
    "            if (len(word_lists[0]) > 1) or (len(word_lists[1]) > 1) or (len(word_lists[2]) > 1):\n",
    "                continue\n",
    "            # 重新整理元组，同时搜集所有出现词\n",
    "            eliments_without_inner_list = [w_l[0].lower() for w_l in word_lists]\n",
    "            newer_dict[key].append((eliments_without_inner_list, full_sent))\n",
    "            get_word_set(eliments_without_inner_list, total_word_set)\n",
    "            \n",
    "    return newer_dict, list(total_word_set)\n",
    "            \n",
    "def get_word_set(tuples, container_set):\n",
    "    \n",
    "    for eliment in tuples:\n",
    "        container_set.add(eliment)\n",
    "    \n",
    "    return True\n",
    "\n",
    "def encoding_process(tuple_dict, encoder): # 将三元组转换为模型可接受的编码\n",
    "        \n",
    "    encoded_tp = []\n",
    "    for species in tqdm(tuple_dict.values()):\n",
    "        for word_lists, _ in species:\n",
    "            encoded_tp.append(encoder.transform(word_lists))\n",
    "        \n",
    "    return encoded_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TupleDataset(Dataset):\n",
    "    # 构建输入神经网路的数据集\n",
    "    def __init__(self, input_data):\n",
    "        self.first_eliment = input_data[0]\n",
    "        self.second_eliment = input_data[1]\n",
    "        self.third_eliment = input_data[2]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.first_eliment)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.first_eliment[idx], self.second_eliment[idx], self.third_eliment[idx])\n",
    "\n",
    "class GraphNet(nn.Module): # 图嵌入网络\n",
    "    \n",
    "    def __init__(self, n_words, embedding_dim=150):\n",
    "        # 初始化\n",
    "        super(GraphNet, self).__init__()\n",
    "        self.embedding_layer1 = nn.Embedding(n_words+1, embedding_dim=embedding_dim)\n",
    "        self.embedding_layer2 = nn.Embedding(n_words+1, embedding_dim=embedding_dim)\n",
    "        self.embedding_layer3 = nn.Embedding(n_words+1, embedding_dim=embedding_dim)\n",
    "        \n",
    "    def forward(self, x, y, z):\n",
    "        # 前向传播\n",
    "        return (self.embedding_layer1(x), self.embedding_layer2(y), self.embedding_layer3(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../datas/tuple/wiki_tri_tuple.pkl\", 'rb') as bf:\n",
    "    tri_tp = pickle.load(bf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14000/14000 [56:58<00:00,  4.10it/s]  \n"
     ]
    }
   ],
   "source": [
    "# 建立单词索引\n",
    "new_dict, total_word_list = word_bag_processer(tri_tp)\n",
    "word_encoder = LabelEncoder()\n",
    "word_encoder.fit(total_word_list)\n",
    "            \n",
    "# 编码\n",
    "input_data = encoding_process(new_dict, word_encoder)\n",
    "input_data = np.expand_dims(np.array(input_data), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datas/encoded/word_encoder.pkl', 'wb') as bfile:\n",
    "    pickle.dump(word_encoder, bfile, protocol=4)\n",
    "np.save('../datas/encoded/encoded_word.npy', input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 0: 3.06641\n",
      "Loss on epoch 1: 3.04126\n",
      "Loss on epoch 2: 3.01627\n",
      "Loss on epoch 3: 2.99145\n",
      "Loss on epoch 4: 2.96680\n",
      "Loss on epoch 5: 2.94232\n",
      "Loss on epoch 6: 2.91802\n",
      "Loss on epoch 7: 2.89389\n",
      "Loss on epoch 8: 2.86993\n",
      "Loss on epoch 9: 2.84616\n",
      "Loss on epoch 10: 2.82256\n",
      "Loss on epoch 11: 2.79915\n",
      "Loss on epoch 12: 2.77592\n",
      "Loss on epoch 13: 2.75286\n",
      "Loss on epoch 14: 2.72999\n",
      "Loss on epoch 15: 2.70731\n",
      "Loss on epoch 16: 2.68480\n",
      "Loss on epoch 17: 2.66248\n",
      "Loss on epoch 18: 2.64034\n",
      "Loss on epoch 19: 2.61838\n",
      "Loss on epoch 20: 2.59661\n",
      "Loss on epoch 21: 2.57502\n",
      "Loss on epoch 22: 2.55361\n",
      "Loss on epoch 23: 2.53238\n",
      "Loss on epoch 24: 2.51133\n",
      "Loss on epoch 25: 2.49047\n",
      "Loss on epoch 26: 2.46978\n",
      "Loss on epoch 27: 2.44927\n",
      "Loss on epoch 28: 2.42894\n",
      "Loss on epoch 29: 2.40879\n",
      "Loss on epoch 30: 2.38882\n",
      "Loss on epoch 31: 2.36902\n",
      "Loss on epoch 32: 2.34939\n",
      "Loss on epoch 33: 2.32994\n",
      "Loss on epoch 34: 2.31066\n",
      "Loss on epoch 35: 2.29155\n",
      "Loss on epoch 36: 2.27261\n",
      "Loss on epoch 37: 2.25384\n",
      "Loss on epoch 38: 2.23524\n",
      "Loss on epoch 39: 2.21680\n",
      "Loss on epoch 40: 2.19853\n",
      "Loss on epoch 41: 2.18042\n",
      "Loss on epoch 42: 2.16247\n",
      "Loss on epoch 43: 2.14468\n",
      "Loss on epoch 44: 2.12705\n",
      "Loss on epoch 45: 2.10957\n",
      "Loss on epoch 46: 2.09226\n",
      "Loss on epoch 47: 2.07509\n",
      "Loss on epoch 48: 2.05809\n",
      "Loss on epoch 49: 2.04123\n",
      "Loss on epoch 50: 2.02452\n",
      "Loss on epoch 51: 2.00796\n",
      "Loss on epoch 52: 1.99155\n",
      "Loss on epoch 53: 1.97529\n",
      "Loss on epoch 54: 1.95917\n",
      "Loss on epoch 55: 1.94319\n",
      "Loss on epoch 56: 1.92736\n",
      "Loss on epoch 57: 1.91166\n",
      "Loss on epoch 58: 1.89611\n",
      "Loss on epoch 59: 1.88069\n",
      "Loss on epoch 60: 1.86541\n",
      "Loss on epoch 61: 1.85026\n",
      "Loss on epoch 62: 1.83525\n",
      "Loss on epoch 63: 1.82037\n",
      "Loss on epoch 64: 1.80562\n",
      "Loss on epoch 65: 1.79101\n",
      "Loss on epoch 66: 1.77652\n",
      "Loss on epoch 67: 1.76216\n",
      "Loss on epoch 68: 1.74792\n",
      "Loss on epoch 69: 1.73381\n",
      "Loss on epoch 70: 1.71982\n",
      "Loss on epoch 71: 1.70596\n",
      "Loss on epoch 72: 1.69222\n",
      "Loss on epoch 73: 1.67859\n",
      "Loss on epoch 74: 1.66509\n",
      "Loss on epoch 75: 1.65171\n",
      "Loss on epoch 76: 1.63844\n",
      "Loss on epoch 77: 1.62528\n",
      "Loss on epoch 78: 1.61224\n",
      "Loss on epoch 79: 1.59932\n",
      "Loss on epoch 80: 1.58650\n",
      "Loss on epoch 81: 1.57380\n",
      "Loss on epoch 82: 1.56121\n",
      "Loss on epoch 83: 1.54873\n",
      "Loss on epoch 84: 1.53635\n",
      "Loss on epoch 85: 1.52408\n",
      "Loss on epoch 86: 1.51192\n",
      "Loss on epoch 87: 1.49986\n",
      "Loss on epoch 88: 1.48791\n",
      "Loss on epoch 89: 1.47605\n",
      "Loss on epoch 90: 1.46430\n",
      "Loss on epoch 91: 1.45266\n",
      "Loss on epoch 92: 1.44111\n",
      "Loss on epoch 93: 1.42966\n",
      "Loss on epoch 94: 1.41830\n",
      "Loss on epoch 95: 1.40705\n",
      "Loss on epoch 96: 1.39589\n",
      "Loss on epoch 97: 1.38482\n",
      "Loss on epoch 98: 1.37385\n",
      "Loss on epoch 99: 1.36298\n",
      "Loss on epoch 100: 1.35219\n",
      "Loss on epoch 101: 1.34150\n",
      "Loss on epoch 102: 1.33090\n",
      "Loss on epoch 103: 1.32039\n",
      "Loss on epoch 104: 1.30997\n",
      "Loss on epoch 105: 1.29963\n",
      "Loss on epoch 106: 1.28938\n",
      "Loss on epoch 107: 1.27922\n",
      "Loss on epoch 108: 1.26915\n",
      "Loss on epoch 109: 1.25916\n",
      "Loss on epoch 110: 1.24925\n",
      "Loss on epoch 111: 1.23943\n",
      "Loss on epoch 112: 1.22969\n",
      "Loss on epoch 113: 1.22003\n",
      "Loss on epoch 114: 1.21046\n",
      "Loss on epoch 115: 1.20096\n",
      "Loss on epoch 116: 1.19154\n",
      "Loss on epoch 117: 1.18220\n",
      "Loss on epoch 118: 1.17294\n",
      "Loss on epoch 119: 1.16376\n",
      "Loss on epoch 120: 1.15465\n",
      "Loss on epoch 121: 1.14562\n",
      "Loss on epoch 122: 1.13667\n",
      "Loss on epoch 123: 1.12779\n",
      "Loss on epoch 124: 1.11898\n",
      "Loss on epoch 125: 1.11025\n",
      "Loss on epoch 126: 1.10159\n",
      "Loss on epoch 127: 1.09300\n",
      "Loss on epoch 128: 1.08448\n",
      "Loss on epoch 129: 1.07604\n",
      "Loss on epoch 130: 1.06766\n",
      "Loss on epoch 131: 1.05935\n",
      "Loss on epoch 132: 1.05111\n",
      "Loss on epoch 133: 1.04294\n",
      "Loss on epoch 134: 1.03484\n",
      "Loss on epoch 135: 1.02680\n",
      "Loss on epoch 136: 1.01883\n",
      "Loss on epoch 137: 1.01093\n",
      "Loss on epoch 138: 1.00309\n",
      "Loss on epoch 139: 0.99531\n",
      "Loss on epoch 140: 0.98760\n",
      "Loss on epoch 141: 0.97995\n",
      "Loss on epoch 142: 0.97237\n",
      "Loss on epoch 143: 0.96484\n",
      "Loss on epoch 144: 0.95738\n",
      "Loss on epoch 145: 0.94998\n",
      "Loss on epoch 146: 0.94264\n",
      "Loss on epoch 147: 0.93536\n",
      "Loss on epoch 148: 0.92814\n",
      "Loss on epoch 149: 0.92098\n",
      "Loss on epoch 150: 0.91388\n",
      "Loss on epoch 151: 0.90683\n",
      "Loss on epoch 152: 0.89984\n",
      "Loss on epoch 153: 0.89291\n",
      "Loss on epoch 154: 0.88604\n",
      "Loss on epoch 155: 0.87922\n",
      "Loss on epoch 156: 0.87245\n",
      "Loss on epoch 157: 0.86574\n",
      "Loss on epoch 158: 0.85909\n",
      "Loss on epoch 159: 0.85249\n",
      "Loss on epoch 160: 0.84594\n",
      "Loss on epoch 161: 0.83944\n",
      "Loss on epoch 162: 0.83300\n",
      "Loss on epoch 163: 0.82661\n",
      "Loss on epoch 164: 0.82027\n",
      "Loss on epoch 165: 0.81399\n",
      "Loss on epoch 166: 0.80775\n",
      "Loss on epoch 167: 0.80156\n",
      "Loss on epoch 168: 0.79542\n",
      "Loss on epoch 169: 0.78934\n",
      "Loss on epoch 170: 0.78330\n",
      "Loss on epoch 171: 0.77731\n",
      "Loss on epoch 172: 0.77136\n",
      "Loss on epoch 173: 0.76547\n",
      "Loss on epoch 174: 0.75962\n",
      "Loss on epoch 175: 0.75382\n",
      "Loss on epoch 176: 0.74807\n",
      "Loss on epoch 177: 0.74236\n",
      "Loss on epoch 178: 0.73670\n",
      "Loss on epoch 179: 0.73108\n",
      "Loss on epoch 180: 0.72550\n",
      "Loss on epoch 181: 0.71998\n",
      "Loss on epoch 182: 0.71449\n",
      "Loss on epoch 183: 0.70905\n",
      "Loss on epoch 184: 0.70365\n",
      "Loss on epoch 185: 0.69830\n",
      "Loss on epoch 186: 0.69299\n",
      "Loss on epoch 187: 0.68771\n",
      "Loss on epoch 188: 0.68249\n",
      "Loss on epoch 189: 0.67730\n",
      "Loss on epoch 190: 0.67215\n",
      "Loss on epoch 191: 0.66705\n",
      "Loss on epoch 192: 0.66198\n",
      "Loss on epoch 193: 0.65696\n",
      "Loss on epoch 194: 0.65197\n",
      "Loss on epoch 195: 0.64703\n",
      "Loss on epoch 196: 0.64212\n",
      "Loss on epoch 197: 0.63725\n",
      "Loss on epoch 198: 0.63242\n",
      "Loss on epoch 199: 0.62763\n",
      "Loss on epoch 200: 0.62288\n",
      "Loss on epoch 201: 0.61816\n",
      "Loss on epoch 202: 0.61348\n",
      "Loss on epoch 203: 0.60884\n",
      "Loss on epoch 204: 0.60423\n",
      "Loss on epoch 205: 0.59966\n",
      "Loss on epoch 206: 0.59513\n",
      "Loss on epoch 207: 0.59063\n",
      "Loss on epoch 208: 0.58616\n",
      "Loss on epoch 209: 0.58173\n",
      "Loss on epoch 210: 0.57734\n",
      "Loss on epoch 211: 0.57298\n",
      "Loss on epoch 212: 0.56865\n",
      "Loss on epoch 213: 0.56436\n",
      "Loss on epoch 214: 0.56010\n",
      "Loss on epoch 215: 0.55588\n",
      "Loss on epoch 216: 0.55169\n",
      "Loss on epoch 217: 0.54753\n",
      "Loss on epoch 218: 0.54340\n",
      "Loss on epoch 219: 0.53930\n",
      "Loss on epoch 220: 0.53524\n",
      "Loss on epoch 221: 0.53121\n",
      "Loss on epoch 222: 0.52721\n",
      "Loss on epoch 223: 0.52324\n",
      "Loss on epoch 224: 0.51930\n",
      "Loss on epoch 225: 0.51539\n",
      "Loss on epoch 226: 0.51151\n",
      "Loss on epoch 227: 0.50766\n",
      "Loss on epoch 228: 0.50385\n",
      "Loss on epoch 229: 0.50006\n",
      "Loss on epoch 230: 0.49630\n",
      "Loss on epoch 231: 0.49257\n",
      "Loss on epoch 232: 0.48886\n",
      "Loss on epoch 233: 0.48519\n",
      "Loss on epoch 234: 0.48155\n",
      "Loss on epoch 235: 0.47793\n",
      "Loss on epoch 236: 0.47434\n",
      "Loss on epoch 237: 0.47078\n",
      "Loss on epoch 238: 0.46725\n",
      "Loss on epoch 239: 0.46374\n",
      "Loss on epoch 240: 0.46026\n",
      "Loss on epoch 241: 0.45681\n",
      "Loss on epoch 242: 0.45338\n",
      "Loss on epoch 243: 0.44998\n",
      "Loss on epoch 244: 0.44661\n",
      "Loss on epoch 245: 0.44326\n",
      "Loss on epoch 246: 0.43994\n",
      "Loss on epoch 247: 0.43664\n",
      "Loss on epoch 248: 0.43337\n",
      "Loss on epoch 249: 0.43012\n",
      "Loss on epoch 250: 0.42690\n",
      "Loss on epoch 251: 0.42370\n",
      "Loss on epoch 252: 0.42053\n",
      "Loss on epoch 253: 0.41738\n",
      "Loss on epoch 254: 0.41426\n",
      "Loss on epoch 255: 0.41116\n",
      "Loss on epoch 256: 0.40808\n",
      "Loss on epoch 257: 0.40503\n",
      "Loss on epoch 258: 0.40200\n",
      "Loss on epoch 259: 0.39899\n",
      "Loss on epoch 260: 0.39601\n",
      "Loss on epoch 261: 0.39304\n",
      "Loss on epoch 262: 0.39010\n",
      "Loss on epoch 263: 0.38719\n",
      "Loss on epoch 264: 0.38429\n",
      "Loss on epoch 265: 0.38142\n",
      "Loss on epoch 266: 0.37857\n",
      "Loss on epoch 267: 0.37574\n",
      "Loss on epoch 268: 0.37293\n",
      "Loss on epoch 269: 0.37015\n",
      "Loss on epoch 270: 0.36738\n",
      "Loss on epoch 271: 0.36464\n",
      "Loss on epoch 272: 0.36192\n",
      "Loss on epoch 273: 0.35921\n",
      "Loss on epoch 274: 0.35653\n",
      "Loss on epoch 275: 0.35387\n",
      "Loss on epoch 276: 0.35123\n",
      "Loss on epoch 277: 0.34861\n",
      "Loss on epoch 278: 0.34600\n",
      "Loss on epoch 279: 0.34342\n",
      "Loss on epoch 280: 0.34086\n",
      "Loss on epoch 281: 0.33832\n",
      "Loss on epoch 282: 0.33579\n",
      "Loss on epoch 283: 0.33329\n",
      "Loss on epoch 284: 0.33080\n",
      "Loss on epoch 285: 0.32833\n",
      "Loss on epoch 286: 0.32588\n",
      "Loss on epoch 287: 0.32345\n",
      "Loss on epoch 288: 0.32104\n",
      "Loss on epoch 289: 0.31865\n",
      "Loss on epoch 290: 0.31627\n",
      "Loss on epoch 291: 0.31391\n",
      "Loss on epoch 292: 0.31157\n",
      "Loss on epoch 293: 0.30925\n",
      "Loss on epoch 294: 0.30694\n",
      "Loss on epoch 295: 0.30466\n",
      "Loss on epoch 296: 0.30239\n",
      "Loss on epoch 297: 0.30013\n",
      "Loss on epoch 298: 0.29789\n",
      "Loss on epoch 299: 0.29567\n",
      "Loss on epoch 300: 0.29347\n",
      "Loss on epoch 301: 0.29128\n",
      "Loss on epoch 302: 0.28911\n",
      "Loss on epoch 303: 0.28696\n",
      "Loss on epoch 304: 0.28482\n",
      "Loss on epoch 305: 0.28270\n",
      "Loss on epoch 306: 0.28059\n",
      "Loss on epoch 307: 0.27850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 308: 0.27643\n",
      "Loss on epoch 309: 0.27437\n",
      "Loss on epoch 310: 0.27232\n",
      "Loss on epoch 311: 0.27030\n",
      "Loss on epoch 312: 0.26828\n",
      "Loss on epoch 313: 0.26628\n",
      "Loss on epoch 314: 0.26430\n",
      "Loss on epoch 315: 0.26233\n",
      "Loss on epoch 316: 0.26038\n",
      "Loss on epoch 317: 0.25844\n",
      "Loss on epoch 318: 0.25651\n",
      "Loss on epoch 319: 0.25460\n",
      "Loss on epoch 320: 0.25271\n",
      "Loss on epoch 321: 0.25083\n",
      "Loss on epoch 322: 0.24896\n",
      "Loss on epoch 323: 0.24710\n",
      "Loss on epoch 324: 0.24526\n",
      "Loss on epoch 325: 0.24344\n",
      "Loss on epoch 326: 0.24162\n",
      "Loss on epoch 327: 0.23982\n",
      "Loss on epoch 328: 0.23804\n",
      "Loss on epoch 329: 0.23626\n",
      "Loss on epoch 330: 0.23451\n",
      "Loss on epoch 331: 0.23276\n",
      "Loss on epoch 332: 0.23102\n",
      "Loss on epoch 333: 0.22930\n",
      "Loss on epoch 334: 0.22760\n",
      "Loss on epoch 335: 0.22590\n",
      "Loss on epoch 336: 0.22422\n",
      "Loss on epoch 337: 0.22255\n",
      "Loss on epoch 338: 0.22089\n",
      "Loss on epoch 339: 0.21925\n",
      "Loss on epoch 340: 0.21761\n",
      "Loss on epoch 341: 0.21599\n",
      "Loss on epoch 342: 0.21438\n",
      "Loss on epoch 343: 0.21278\n",
      "Loss on epoch 344: 0.21120\n",
      "Loss on epoch 345: 0.20963\n",
      "Loss on epoch 346: 0.20806\n",
      "Loss on epoch 347: 0.20651\n",
      "Loss on epoch 348: 0.20498\n",
      "Loss on epoch 349: 0.20345\n",
      "Loss on epoch 350: 0.20193\n",
      "Loss on epoch 351: 0.20043\n",
      "Loss on epoch 352: 0.19893\n",
      "Loss on epoch 353: 0.19745\n",
      "Loss on epoch 354: 0.19598\n",
      "Loss on epoch 355: 0.19452\n",
      "Loss on epoch 356: 0.19307\n",
      "Loss on epoch 357: 0.19163\n",
      "Loss on epoch 358: 0.19020\n",
      "Loss on epoch 359: 0.18878\n",
      "Loss on epoch 360: 0.18738\n",
      "Loss on epoch 361: 0.18598\n",
      "Loss on epoch 362: 0.18459\n",
      "Loss on epoch 363: 0.18322\n",
      "Loss on epoch 364: 0.18185\n",
      "Loss on epoch 365: 0.18049\n",
      "Loss on epoch 366: 0.17915\n",
      "Loss on epoch 367: 0.17781\n",
      "Loss on epoch 368: 0.17648\n",
      "Loss on epoch 369: 0.17517\n",
      "Loss on epoch 370: 0.17386\n",
      "Loss on epoch 371: 0.17256\n",
      "Loss on epoch 372: 0.17128\n",
      "Loss on epoch 373: 0.17000\n",
      "Loss on epoch 374: 0.16873\n",
      "Loss on epoch 375: 0.16747\n",
      "Loss on epoch 376: 0.16622\n",
      "Loss on epoch 377: 0.16498\n",
      "Loss on epoch 378: 0.16375\n",
      "Loss on epoch 379: 0.16252\n",
      "Loss on epoch 380: 0.16131\n",
      "Loss on epoch 381: 0.16011\n",
      "Loss on epoch 382: 0.15891\n",
      "Loss on epoch 383: 0.15772\n",
      "Loss on epoch 384: 0.15655\n",
      "Loss on epoch 385: 0.15538\n",
      "Loss on epoch 386: 0.15422\n",
      "Loss on epoch 387: 0.15306\n",
      "Loss on epoch 388: 0.15192\n",
      "Loss on epoch 389: 0.15079\n",
      "Loss on epoch 390: 0.14966\n",
      "Loss on epoch 391: 0.14854\n",
      "Loss on epoch 392: 0.14743\n",
      "Loss on epoch 393: 0.14633\n",
      "Loss on epoch 394: 0.14523\n",
      "Loss on epoch 395: 0.14415\n",
      "Loss on epoch 396: 0.14307\n",
      "Loss on epoch 397: 0.14200\n",
      "Loss on epoch 398: 0.14094\n",
      "Loss on epoch 399: 0.13988\n",
      "Loss on epoch 400: 0.13884\n",
      "Loss on epoch 401: 0.13780\n",
      "Loss on epoch 402: 0.13677\n",
      "Loss on epoch 403: 0.13574\n",
      "Loss on epoch 404: 0.13473\n",
      "Loss on epoch 405: 0.13372\n",
      "Loss on epoch 406: 0.13272\n",
      "Loss on epoch 407: 0.13172\n",
      "Loss on epoch 408: 0.13074\n",
      "Loss on epoch 409: 0.12976\n",
      "Loss on epoch 410: 0.12879\n",
      "Loss on epoch 411: 0.12782\n",
      "Loss on epoch 412: 0.12686\n",
      "Loss on epoch 413: 0.12591\n",
      "Loss on epoch 414: 0.12497\n",
      "Loss on epoch 415: 0.12403\n",
      "Loss on epoch 416: 0.12310\n",
      "Loss on epoch 417: 0.12218\n",
      "Loss on epoch 418: 0.12126\n",
      "Loss on epoch 419: 0.12035\n",
      "Loss on epoch 420: 0.11945\n",
      "Loss on epoch 421: 0.11855\n",
      "Loss on epoch 422: 0.11767\n",
      "Loss on epoch 423: 0.11678\n",
      "Loss on epoch 424: 0.11591\n",
      "Loss on epoch 425: 0.11504\n",
      "Loss on epoch 426: 0.11417\n",
      "Loss on epoch 427: 0.11332\n",
      "Loss on epoch 428: 0.11246\n",
      "Loss on epoch 429: 0.11162\n",
      "Loss on epoch 430: 0.11078\n",
      "Loss on epoch 431: 0.10995\n",
      "Loss on epoch 432: 0.10912\n",
      "Loss on epoch 433: 0.10830\n",
      "Loss on epoch 434: 0.10749\n",
      "Loss on epoch 435: 0.10668\n",
      "Loss on epoch 436: 0.10588\n",
      "Loss on epoch 437: 0.10508\n",
      "Loss on epoch 438: 0.10429\n",
      "Loss on epoch 439: 0.10351\n",
      "Loss on epoch 440: 0.10273\n",
      "Loss on epoch 441: 0.10196\n",
      "Loss on epoch 442: 0.10119\n",
      "Loss on epoch 443: 0.10043\n",
      "Loss on epoch 444: 0.09967\n",
      "Loss on epoch 445: 0.09892\n",
      "Loss on epoch 446: 0.09818\n",
      "Loss on epoch 447: 0.09744\n",
      "Loss on epoch 448: 0.09670\n",
      "Loss on epoch 449: 0.09597\n",
      "Loss on epoch 450: 0.09525\n",
      "Loss on epoch 451: 0.09453\n",
      "Loss on epoch 452: 0.09382\n",
      "Loss on epoch 453: 0.09311\n",
      "Loss on epoch 454: 0.09241\n",
      "Loss on epoch 455: 0.09172\n",
      "Loss on epoch 456: 0.09102\n",
      "Loss on epoch 457: 0.09034\n",
      "Loss on epoch 458: 0.08966\n",
      "Loss on epoch 459: 0.08898\n",
      "Loss on epoch 460: 0.08831\n",
      "Loss on epoch 461: 0.08764\n",
      "Loss on epoch 462: 0.08698\n",
      "Loss on epoch 463: 0.08632\n",
      "Loss on epoch 464: 0.08567\n",
      "Loss on epoch 465: 0.08502\n",
      "Loss on epoch 466: 0.08438\n",
      "Loss on epoch 467: 0.08374\n",
      "Loss on epoch 468: 0.08311\n",
      "Loss on epoch 469: 0.08248\n",
      "Loss on epoch 470: 0.08186\n",
      "Loss on epoch 471: 0.08124\n",
      "Loss on epoch 472: 0.08062\n",
      "Loss on epoch 473: 0.08001\n",
      "Loss on epoch 474: 0.07941\n",
      "Loss on epoch 475: 0.07881\n",
      "Loss on epoch 476: 0.07821\n",
      "Loss on epoch 477: 0.07762\n",
      "Loss on epoch 478: 0.07703\n",
      "Loss on epoch 479: 0.07645\n",
      "Loss on epoch 480: 0.07587\n",
      "Loss on epoch 481: 0.07529\n",
      "Loss on epoch 482: 0.07472\n",
      "Loss on epoch 483: 0.07416\n",
      "Loss on epoch 484: 0.07359\n",
      "Loss on epoch 485: 0.07304\n",
      "Loss on epoch 486: 0.07248\n",
      "Loss on epoch 487: 0.07193\n",
      "Loss on epoch 488: 0.07139\n",
      "Loss on epoch 489: 0.07084\n",
      "Loss on epoch 490: 0.07031\n",
      "Loss on epoch 491: 0.06977\n",
      "Loss on epoch 492: 0.06924\n",
      "Loss on epoch 493: 0.06872\n",
      "Loss on epoch 494: 0.06819\n",
      "Loss on epoch 495: 0.06768\n",
      "Loss on epoch 496: 0.06716\n",
      "Loss on epoch 497: 0.06665\n",
      "Loss on epoch 498: 0.06614\n",
      "Loss on epoch 499: 0.06564\n",
      "Loss on epoch 500: 0.06514\n",
      "Loss on epoch 501: 0.06465\n",
      "Loss on epoch 502: 0.06415\n",
      "Loss on epoch 503: 0.06367\n",
      "Loss on epoch 504: 0.06318\n",
      "Loss on epoch 505: 0.06270\n",
      "Loss on epoch 506: 0.06222\n",
      "Loss on epoch 507: 0.06175\n",
      "Loss on epoch 508: 0.06128\n",
      "Loss on epoch 509: 0.06081\n",
      "Loss on epoch 510: 0.06035\n",
      "Loss on epoch 511: 0.05989\n",
      "Loss on epoch 512: 0.05943\n",
      "Loss on epoch 513: 0.05898\n",
      "Loss on epoch 514: 0.05853\n",
      "Loss on epoch 515: 0.05808\n",
      "Loss on epoch 516: 0.05764\n",
      "Loss on epoch 517: 0.05720\n",
      "Loss on epoch 518: 0.05676\n",
      "Loss on epoch 519: 0.05633\n",
      "Loss on epoch 520: 0.05590\n",
      "Loss on epoch 521: 0.05547\n",
      "Loss on epoch 522: 0.05505\n",
      "Loss on epoch 523: 0.05463\n",
      "Loss on epoch 524: 0.05421\n",
      "Loss on epoch 525: 0.05379\n",
      "Loss on epoch 526: 0.05338\n",
      "Loss on epoch 527: 0.05297\n",
      "Loss on epoch 528: 0.05257\n",
      "Loss on epoch 529: 0.05217\n",
      "Loss on epoch 530: 0.05177\n",
      "Loss on epoch 531: 0.05137\n",
      "Loss on epoch 532: 0.05098\n",
      "Loss on epoch 533: 0.05059\n",
      "Loss on epoch 534: 0.05020\n",
      "Loss on epoch 535: 0.04981\n",
      "Loss on epoch 536: 0.04943\n",
      "Loss on epoch 537: 0.04905\n",
      "Loss on epoch 538: 0.04868\n",
      "Loss on epoch 539: 0.04830\n",
      "Loss on epoch 540: 0.04793\n",
      "Loss on epoch 541: 0.04756\n",
      "Loss on epoch 542: 0.04720\n",
      "Loss on epoch 543: 0.04684\n",
      "Loss on epoch 544: 0.04648\n",
      "Loss on epoch 545: 0.04612\n",
      "Loss on epoch 546: 0.04576\n",
      "Loss on epoch 547: 0.04541\n",
      "Loss on epoch 548: 0.04506\n",
      "Loss on epoch 549: 0.04472\n",
      "Loss on epoch 550: 0.04437\n",
      "Loss on epoch 551: 0.04403\n",
      "Loss on epoch 552: 0.04369\n",
      "Loss on epoch 553: 0.04336\n",
      "Loss on epoch 554: 0.04302\n",
      "Loss on epoch 555: 0.04269\n",
      "Loss on epoch 556: 0.04236\n",
      "Loss on epoch 557: 0.04204\n",
      "Loss on epoch 558: 0.04171\n",
      "Loss on epoch 559: 0.04139\n",
      "Loss on epoch 560: 0.04107\n",
      "Loss on epoch 561: 0.04075\n",
      "Loss on epoch 562: 0.04044\n",
      "Loss on epoch 563: 0.04013\n",
      "Loss on epoch 564: 0.03982\n",
      "Loss on epoch 565: 0.03951\n",
      "Loss on epoch 566: 0.03921\n",
      "Loss on epoch 567: 0.03890\n",
      "Loss on epoch 568: 0.03860\n",
      "Loss on epoch 569: 0.03830\n",
      "Loss on epoch 570: 0.03801\n",
      "Loss on epoch 571: 0.03771\n",
      "Loss on epoch 572: 0.03742\n",
      "Loss on epoch 573: 0.03713\n",
      "Loss on epoch 574: 0.03685\n",
      "Loss on epoch 575: 0.03656\n",
      "Loss on epoch 576: 0.03628\n",
      "Loss on epoch 577: 0.03600\n",
      "Loss on epoch 578: 0.03572\n",
      "Loss on epoch 579: 0.03544\n",
      "Loss on epoch 580: 0.03517\n",
      "Loss on epoch 581: 0.03489\n",
      "Loss on epoch 582: 0.03462\n",
      "Loss on epoch 583: 0.03435\n",
      "Loss on epoch 584: 0.03409\n",
      "Loss on epoch 585: 0.03382\n",
      "Loss on epoch 586: 0.03356\n",
      "Loss on epoch 587: 0.03330\n",
      "Loss on epoch 588: 0.03304\n",
      "Loss on epoch 589: 0.03278\n",
      "Loss on epoch 590: 0.03253\n",
      "Loss on epoch 591: 0.03228\n",
      "Loss on epoch 592: 0.03203\n",
      "Loss on epoch 593: 0.03178\n",
      "Loss on epoch 594: 0.03153\n",
      "Loss on epoch 595: 0.03128\n",
      "Loss on epoch 596: 0.03104\n",
      "Loss on epoch 597: 0.03080\n",
      "Loss on epoch 598: 0.03056\n",
      "Loss on epoch 599: 0.03032\n",
      "Loss on epoch 600: 0.03009\n",
      "Loss on epoch 601: 0.02985\n",
      "Loss on epoch 602: 0.02962\n",
      "Loss on epoch 603: 0.02939\n",
      "Loss on epoch 604: 0.02916\n",
      "Loss on epoch 605: 0.02893\n",
      "Loss on epoch 606: 0.02871\n",
      "Loss on epoch 607: 0.02848\n",
      "Loss on epoch 608: 0.02826\n",
      "Loss on epoch 609: 0.02804\n",
      "Loss on epoch 610: 0.02782\n",
      "Loss on epoch 611: 0.02760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 612: 0.02739\n",
      "Loss on epoch 613: 0.02717\n",
      "Loss on epoch 614: 0.02696\n",
      "Loss on epoch 615: 0.02675\n",
      "Loss on epoch 616: 0.02654\n",
      "Loss on epoch 617: 0.02633\n",
      "Loss on epoch 618: 0.02613\n",
      "Loss on epoch 619: 0.02592\n",
      "Loss on epoch 620: 0.02572\n",
      "Loss on epoch 621: 0.02552\n",
      "Loss on epoch 622: 0.02532\n",
      "Loss on epoch 623: 0.02512\n",
      "Loss on epoch 624: 0.02492\n",
      "Loss on epoch 625: 0.02473\n",
      "Loss on epoch 626: 0.02453\n",
      "Loss on epoch 627: 0.02434\n",
      "Loss on epoch 628: 0.02415\n",
      "Loss on epoch 629: 0.02396\n",
      "Loss on epoch 630: 0.02377\n",
      "Loss on epoch 631: 0.02358\n",
      "Loss on epoch 632: 0.02340\n",
      "Loss on epoch 633: 0.02321\n",
      "Loss on epoch 634: 0.02303\n",
      "Loss on epoch 635: 0.02285\n",
      "Loss on epoch 636: 0.02267\n",
      "Loss on epoch 637: 0.02249\n",
      "Loss on epoch 638: 0.02232\n",
      "Loss on epoch 639: 0.02214\n",
      "Loss on epoch 640: 0.02197\n",
      "Loss on epoch 641: 0.02179\n",
      "Loss on epoch 642: 0.02162\n",
      "Loss on epoch 643: 0.02145\n",
      "Loss on epoch 644: 0.02128\n",
      "Loss on epoch 645: 0.02111\n",
      "Loss on epoch 646: 0.02095\n",
      "Loss on epoch 647: 0.02078\n",
      "Loss on epoch 648: 0.02062\n",
      "Loss on epoch 649: 0.02046\n",
      "Loss on epoch 650: 0.02029\n",
      "Loss on epoch 651: 0.02013\n",
      "Loss on epoch 652: 0.01997\n",
      "Loss on epoch 653: 0.01982\n",
      "Loss on epoch 654: 0.01966\n",
      "Loss on epoch 655: 0.01950\n",
      "Loss on epoch 656: 0.01935\n",
      "Loss on epoch 657: 0.01920\n",
      "Loss on epoch 658: 0.01905\n",
      "Loss on epoch 659: 0.01889\n",
      "Loss on epoch 660: 0.01874\n",
      "Loss on epoch 661: 0.01860\n",
      "Loss on epoch 662: 0.01845\n",
      "Loss on epoch 663: 0.01830\n",
      "Loss on epoch 664: 0.01816\n",
      "Loss on epoch 665: 0.01801\n",
      "Loss on epoch 666: 0.01787\n",
      "Loss on epoch 667: 0.01773\n",
      "Loss on epoch 668: 0.01759\n",
      "Loss on epoch 669: 0.01745\n",
      "Loss on epoch 670: 0.01731\n",
      "Loss on epoch 671: 0.01717\n",
      "Loss on epoch 672: 0.01704\n",
      "Loss on epoch 673: 0.01690\n",
      "Loss on epoch 674: 0.01677\n",
      "Loss on epoch 675: 0.01663\n",
      "Loss on epoch 676: 0.01650\n",
      "Loss on epoch 677: 0.01637\n",
      "Loss on epoch 678: 0.01624\n",
      "Loss on epoch 679: 0.01611\n",
      "Loss on epoch 680: 0.01598\n",
      "Loss on epoch 681: 0.01585\n",
      "Loss on epoch 682: 0.01573\n",
      "Loss on epoch 683: 0.01560\n",
      "Loss on epoch 684: 0.01548\n",
      "Loss on epoch 685: 0.01535\n",
      "Loss on epoch 686: 0.01523\n",
      "Loss on epoch 687: 0.01511\n",
      "Loss on epoch 688: 0.01499\n",
      "Loss on epoch 689: 0.01487\n",
      "Loss on epoch 690: 0.01475\n",
      "Loss on epoch 691: 0.01463\n",
      "Loss on epoch 692: 0.01452\n",
      "Loss on epoch 693: 0.01440\n",
      "Loss on epoch 694: 0.01429\n",
      "Loss on epoch 695: 0.01417\n",
      "Loss on epoch 696: 0.01406\n",
      "Loss on epoch 697: 0.01395\n",
      "Loss on epoch 698: 0.01383\n",
      "Loss on epoch 699: 0.01372\n",
      "Loss on epoch 700: 0.01361\n",
      "Loss on epoch 701: 0.01350\n",
      "Loss on epoch 702: 0.01340\n",
      "Loss on epoch 703: 0.01329\n",
      "Loss on epoch 704: 0.01318\n",
      "Loss on epoch 705: 0.01308\n",
      "Loss on epoch 706: 0.01297\n",
      "Loss on epoch 707: 0.01287\n",
      "Loss on epoch 708: 0.01276\n",
      "Loss on epoch 709: 0.01266\n",
      "Loss on epoch 710: 0.01256\n",
      "Loss on epoch 711: 0.01246\n",
      "Loss on epoch 712: 0.01236\n",
      "Loss on epoch 713: 0.01226\n",
      "Loss on epoch 714: 0.01216\n",
      "Loss on epoch 715: 0.01206\n",
      "Loss on epoch 716: 0.01197\n",
      "Loss on epoch 717: 0.01187\n",
      "Loss on epoch 718: 0.01177\n",
      "Loss on epoch 719: 0.01168\n",
      "Loss on epoch 720: 0.01158\n",
      "Loss on epoch 721: 0.01149\n",
      "Loss on epoch 722: 0.01140\n",
      "Loss on epoch 723: 0.01131\n",
      "Loss on epoch 724: 0.01122\n",
      "Loss on epoch 725: 0.01112\n",
      "Loss on epoch 726: 0.01103\n",
      "Loss on epoch 727: 0.01095\n",
      "Loss on epoch 728: 0.01086\n",
      "Loss on epoch 729: 0.01077\n",
      "Loss on epoch 730: 0.01068\n",
      "Loss on epoch 731: 0.01060\n",
      "Loss on epoch 732: 0.01051\n",
      "Loss on epoch 733: 0.01043\n",
      "Loss on epoch 734: 0.01034\n",
      "Loss on epoch 735: 0.01026\n",
      "Loss on epoch 736: 0.01017\n",
      "Loss on epoch 737: 0.01009\n",
      "Loss on epoch 738: 0.01001\n",
      "Loss on epoch 739: 0.00993\n",
      "Loss on epoch 740: 0.00985\n",
      "Loss on epoch 741: 0.00977\n",
      "Loss on epoch 742: 0.00969\n",
      "Loss on epoch 743: 0.00961\n",
      "Loss on epoch 744: 0.00953\n",
      "Loss on epoch 745: 0.00946\n",
      "Loss on epoch 746: 0.00938\n",
      "Loss on epoch 747: 0.00930\n",
      "Loss on epoch 748: 0.00923\n",
      "Loss on epoch 749: 0.00915\n",
      "Loss on epoch 750: 0.00908\n",
      "Loss on epoch 751: 0.00900\n",
      "Loss on epoch 752: 0.00893\n",
      "Loss on epoch 753: 0.00886\n",
      "Loss on epoch 754: 0.00879\n",
      "Loss on epoch 755: 0.00871\n",
      "Loss on epoch 756: 0.00864\n",
      "Loss on epoch 757: 0.00857\n",
      "Loss on epoch 758: 0.00850\n",
      "Loss on epoch 759: 0.00843\n",
      "Loss on epoch 760: 0.00836\n",
      "Loss on epoch 761: 0.00830\n",
      "Loss on epoch 762: 0.00823\n",
      "Loss on epoch 763: 0.00816\n",
      "Loss on epoch 764: 0.00809\n",
      "Loss on epoch 765: 0.00803\n",
      "Loss on epoch 766: 0.00796\n",
      "Loss on epoch 767: 0.00790\n",
      "Loss on epoch 768: 0.00783\n",
      "Loss on epoch 769: 0.00777\n",
      "Loss on epoch 770: 0.00771\n",
      "Loss on epoch 771: 0.00764\n",
      "Loss on epoch 772: 0.00758\n",
      "Loss on epoch 773: 0.00752\n",
      "Loss on epoch 774: 0.00746\n",
      "Loss on epoch 775: 0.00740\n",
      "Loss on epoch 776: 0.00733\n",
      "Loss on epoch 777: 0.00727\n",
      "Loss on epoch 778: 0.00721\n",
      "Loss on epoch 779: 0.00716\n",
      "Loss on epoch 780: 0.00710\n",
      "Loss on epoch 781: 0.00704\n",
      "Loss on epoch 782: 0.00698\n",
      "Loss on epoch 783: 0.00692\n",
      "Loss on epoch 784: 0.00687\n",
      "Loss on epoch 785: 0.00681\n",
      "Loss on epoch 786: 0.00675\n",
      "Loss on epoch 787: 0.00670\n",
      "Loss on epoch 788: 0.00664\n",
      "Loss on epoch 789: 0.00659\n",
      "Loss on epoch 790: 0.00653\n",
      "Loss on epoch 791: 0.00648\n",
      "Loss on epoch 792: 0.00643\n",
      "Loss on epoch 793: 0.00637\n",
      "Loss on epoch 794: 0.00632\n",
      "Loss on epoch 795: 0.00627\n",
      "Loss on epoch 796: 0.00622\n",
      "Loss on epoch 797: 0.00617\n",
      "Loss on epoch 798: 0.00612\n",
      "Loss on epoch 799: 0.00607\n",
      "Loss on epoch 800: 0.00602\n",
      "Loss on epoch 801: 0.00597\n",
      "Loss on epoch 802: 0.00592\n",
      "Loss on epoch 803: 0.00587\n",
      "Loss on epoch 804: 0.00582\n",
      "Loss on epoch 805: 0.00577\n",
      "Loss on epoch 806: 0.00572\n",
      "Loss on epoch 807: 0.00568\n",
      "Loss on epoch 808: 0.00563\n",
      "Loss on epoch 809: 0.00558\n",
      "Loss on epoch 810: 0.00554\n",
      "Loss on epoch 811: 0.00549\n",
      "Loss on epoch 812: 0.00544\n",
      "Loss on epoch 813: 0.00540\n",
      "Loss on epoch 814: 0.00535\n",
      "Loss on epoch 815: 0.00531\n",
      "Loss on epoch 816: 0.00527\n",
      "Loss on epoch 817: 0.00522\n",
      "Loss on epoch 818: 0.00518\n",
      "Loss on epoch 819: 0.00514\n",
      "Loss on epoch 820: 0.00509\n",
      "Loss on epoch 821: 0.00505\n",
      "Loss on epoch 822: 0.00501\n",
      "Loss on epoch 823: 0.00497\n",
      "Loss on epoch 824: 0.00493\n",
      "Loss on epoch 825: 0.00488\n",
      "Loss on epoch 826: 0.00484\n",
      "Loss on epoch 827: 0.00480\n",
      "Loss on epoch 828: 0.00476\n",
      "Loss on epoch 829: 0.00472\n",
      "Loss on epoch 830: 0.00468\n",
      "Loss on epoch 831: 0.00465\n",
      "Loss on epoch 832: 0.00461\n",
      "Loss on epoch 833: 0.00457\n",
      "Loss on epoch 834: 0.00453\n",
      "Loss on epoch 835: 0.00449\n",
      "Loss on epoch 836: 0.00445\n",
      "Loss on epoch 837: 0.00442\n",
      "Loss on epoch 838: 0.00438\n",
      "Loss on epoch 839: 0.00434\n",
      "Loss on epoch 840: 0.00431\n",
      "Loss on epoch 841: 0.00427\n",
      "Loss on epoch 842: 0.00424\n",
      "Loss on epoch 843: 0.00420\n",
      "Loss on epoch 844: 0.00417\n",
      "Loss on epoch 845: 0.00413\n",
      "Loss on epoch 846: 0.00410\n",
      "Loss on epoch 847: 0.00406\n",
      "Loss on epoch 848: 0.00403\n",
      "Loss on epoch 849: 0.00399\n",
      "Loss on epoch 850: 0.00396\n",
      "Loss on epoch 851: 0.00393\n",
      "Loss on epoch 852: 0.00389\n",
      "Loss on epoch 853: 0.00386\n",
      "Loss on epoch 854: 0.00383\n",
      "Loss on epoch 855: 0.00380\n",
      "Loss on epoch 856: 0.00376\n",
      "Loss on epoch 857: 0.00373\n",
      "Loss on epoch 858: 0.00370\n",
      "Loss on epoch 859: 0.00367\n",
      "Loss on epoch 860: 0.00364\n",
      "Loss on epoch 861: 0.00361\n",
      "Loss on epoch 862: 0.00358\n",
      "Loss on epoch 863: 0.00355\n",
      "Loss on epoch 864: 0.00352\n"
     ]
    }
   ],
   "source": [
    "# 数据输入准备\n",
    "tuple_dataset = TupleDataset(input_data)\n",
    "tuple_loader = DataLoader(tuple_dataset, batch_size=128, shuffle=True, num_workers=multiprocessing.cpu_count())\n",
    "\n",
    "net = GraphNet(len(word_encoder.classes_)).to('cuda')\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=3e-3)\n",
    "mse_loss = nn.MSELoss().to(\"cuda\")\n",
    "\n",
    "# 训练（使用GPU）\n",
    "EPOCHS = 30000\n",
    "eta =  3e-5# 早停步长\n",
    "mean_mse = 0\n",
    "for epk in range(EPOCHS):\n",
    "    last_mse = mean_mse\n",
    "    mean_mse = 0\n",
    "    for pos, (x, y, z) in enumerate(tuple_loader):\n",
    "        x = x.long().to('cuda')\n",
    "        y = y.long().to('cuda')\n",
    "        z = z.long().to('cuda')\n",
    "        \n",
    "        optimizer.zero_grad() #优化器梯度清零\n",
    "        x, y, z = net(x, y, z) #前向传播\n",
    "        loss = mse_loss(x+y, z) #损失函数\n",
    "        loss.backward() #反向传播\n",
    "        optimizer.step() #更新参数\n",
    "        \n",
    "        mean_mse += loss\n",
    "    mean_mse /= (pos + 1)\n",
    "    print(\"Loss on epoch %d: %.5f\" % (epk, mean_mse))\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    if (last_mse - mean_mse < eta) & (epk > 20):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"../models/graphembedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphNet(\n",
       "  (embedding_layer1): Embedding(36038, 150)\n",
       "  (embedding_layer2): Embedding(36038, 150)\n",
       "  (embedding_layer3): Embedding(36038, 150)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取模型\n",
    "model = torch.load(\"../models/graphembedding\")\n",
    "net = GraphNet(len(word_encoder.classes_))\n",
    "net.load_state_dict(model)\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_raw_vector_dict = defaultdict(list)\n",
    "label_ptr = 0\n",
    "input_data = torch.from_numpy(input_data).long().to('cuda')\n",
    "\n",
    "# 三元组向量化\n",
    "net = net.to('cuda')\n",
    "with torch.no_grad():\n",
    "    x, y, z = net(input_data[:, 0], input_data[:, 1], input_data[:, 2])\n",
    "\n",
    "x = np.squeeze(x.to('cpu').numpy(), axis=1)\n",
    "y = np.squeeze(y.to('cpu').numpy(), axis=1)\n",
    "z = np.squeeze(z.to('cpu').numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 记录完整图谱\n",
    "tuple_yielder = zip(x, y, z)\n",
    "tuple_raw_vector_dict = defaultdict(list)\n",
    "\n",
    "for key, value in new_dict.items():\n",
    "    for t, s in value:\n",
    "        tuple_raw_vector_dict[key].append([t, s, next(iter(tuple_yielder))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../datas/knowledge/graph.pkl\", 'wb') as g:\n",
    "    pickle.dump(tuple_raw_vector_dict, g, protocol=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
