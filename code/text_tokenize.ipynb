{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import copy\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(path):    # 读取文本\n",
    "\n",
    "    bio_dict = {}\n",
    "    \n",
    "    with open(path) as f:\n",
    "        raw_text = json.load(f)\n",
    "    json_text = json.loads(raw_text)\n",
    "    \n",
    "    json_text = list(json_text.values())[0]\n",
    "    for page_num in range(len(json_text)):\n",
    "        species = list(json_text[page_num].keys())[0]\n",
    "        text = list(json_text[page_num].values())[0]['text']\n",
    "        bio_dict[species] = text\n",
    "        \n",
    "    return bio_dict\n",
    "\n",
    "def file_preprocess(text):    # 文本预处理\n",
    "    \n",
    "    regex_ws=re.compile(\"\\s+\")\n",
    "    enter = re.compile('\\n+')\n",
    "    reference = re.compile('\\[\\d+\\]')\n",
    "    html = re.compile(r'<.*?>')\n",
    "    url = re.compile(\"(https?:\\/\\/(?:www\\.|(?!www)|(?:xmlns\\.))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\")\n",
    "    email = re.compile('@[A-z0-9_]+')\n",
    "    num = re.compile(r\"(\\s\\d+)\")\n",
    "\n",
    "    text = text.replace(\"&amp;\",\"&\").replace(\"&lt;\",\"<\").replace(\"&gt;\",\">\").replace(\"%20\", \" \")\n",
    "    text = enter.sub(\" \", text)\n",
    "    text = regex_ws.sub(\" \", text)\n",
    "    text = reference.sub(\" \", text)\n",
    "    text = regex_ws.sub(\" \", text)\n",
    "    text = html.sub(\" \", text)\n",
    "    text = regex_ws.sub(\" \", text)\n",
    "    text = url.sub(\" \", text)\n",
    "    text = regex_ws.sub(\" \", text)\n",
    "    text = email.sub(\" \", text)\n",
    "    text = regex_ws.sub(\" \", text)\n",
    "    text = num.sub(\" \", text)\n",
    "    text = regex_ws.sub(\" \", text)\n",
    "\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"there's\", \"there is\", text)\n",
    "    text = re.sub(r\"We're\", \"We are\", text)\n",
    "    text = re.sub(r\"That's\", \"That is\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"they're\", \"they are\", text)\n",
    "    text = re.sub(r\"Can't\", \"Cannot\", text)\n",
    "    text = re.sub(r\"wasn't\", \"was not\", text)\n",
    "    text = re.sub(r\"don\\x89Ûªt\", \"do not\", text)\n",
    "    text = re.sub(r\"aren't\", \"are not\", text)\n",
    "    text = re.sub(r\"isn't\", \"is not\", text)\n",
    "    text = re.sub(r\"What's\", \"What is\", text)\n",
    "    text = re.sub(r\"haven't\", \"have not\", text)\n",
    "    text = re.sub(r\"hasn't\", \"has not\", text)\n",
    "    text = re.sub(r\"There's\", \"There is\", text)\n",
    "    text = re.sub(r\"He's\", \"He is\", text)\n",
    "    text = re.sub(r\"It's\", \"It is\", text)\n",
    "    text = re.sub(r\"You're\", \"You are\", text)\n",
    "    text = re.sub(r\"I'M\", \"I am\", text)\n",
    "    text = re.sub(r\"shouldn't\", \"should not\", text)\n",
    "    text = re.sub(r\"wouldn't\", \"would not\", text)\n",
    "    text = re.sub(r\"i'm\", \"I am\", text)\n",
    "    text = re.sub(r\"I\\x89Ûªm\", \"I am\", text)\n",
    "    text = re.sub(r\"I'm\", \"I am\", text)\n",
    "    text = re.sub(r\"Isn't\", \"is not\", text)\n",
    "    text = re.sub(r\"Here's\", \"Here is\", text)\n",
    "    text = re.sub(r\"you've\", \"you have\", text)\n",
    "    text = re.sub(r\"you\\x89Ûªve\", \"you have\", text)\n",
    "    text = re.sub(r\"we're\", \"we are\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"couldn't\", \"could not\", text)\n",
    "    text = re.sub(r\"we've\", \"we have\", text)\n",
    "    text = re.sub(r\"it\\x89Ûªs\", \"it is\", text)\n",
    "    text = re.sub(r\"doesn\\x89Ûªt\", \"does not\", text)\n",
    "    text = re.sub(r\"It\\x89Ûªs\", \"It is\", text)\n",
    "    text = re.sub(r\"Here\\x89Ûªs\", \"Here is\", text)\n",
    "    text = re.sub(r\"who's\", \"who is\", text)\n",
    "    text = re.sub(r\"I\\x89Ûªve\", \"I have\", text)\n",
    "    text = re.sub(r\"y'all\", \"you all\", text)\n",
    "    text = re.sub(r\"can\\x89Ûªt\", \"cannot\", text)\n",
    "    text = re.sub(r\"would've\", \"would have\", text)\n",
    "    text = re.sub(r\"it'll\", \"it will\", text)\n",
    "    text = re.sub(r\"we'll\", \"we will\", text)\n",
    "    text = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", text)\n",
    "    text = re.sub(r\"We've\", \"We have\", text)\n",
    "    text = re.sub(r\"he'll\", \"he will\", text)\n",
    "    text = re.sub(r\"Y'all\", \"You all\", text)\n",
    "    text = re.sub(r\"Weren't\", \"Were not\", text)\n",
    "    text = re.sub(r\"Didn't\", \"Did not\", text)\n",
    "    text = re.sub(r\"they'll\", \"they will\", text)\n",
    "    text = re.sub(r\"they'd\", \"they would\", text)\n",
    "    text = re.sub(r\"DON'T\", \"DO NOT\", text)\n",
    "    text = re.sub(r\"That\\x89Ûªs\", \"That is\", text)\n",
    "    text = re.sub(r\"they've\", \"they have\", text)\n",
    "    text = re.sub(r\"i'd\", \"I would\", text)\n",
    "    text = re.sub(r\"should've\", \"should have\", text)\n",
    "    text = re.sub(r\"You\\x89Ûªre\", \"You are\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"Don\\x89Ûªt\", \"Do not\", text)\n",
    "    text = re.sub(r\"we'd\", \"we would\", text)\n",
    "    text = re.sub(r\"i'll\", \"I will\", text)\n",
    "    text = re.sub(r\"weren't\", \"were not\", text)\n",
    "    text = re.sub(r\"They're\", \"They are\", text)\n",
    "    text = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", text)\n",
    "    text = re.sub(r\"you\\x89Ûªll\", \"you will\", text)\n",
    "    text = re.sub(r\"I\\x89Ûªd\", \"I would\", text)\n",
    "    text = re.sub(r\"let's\", \"let us\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"you're\", \"you are\", text)\n",
    "    text = re.sub(r\"i've\", \"I have\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"i'll\", \"I will\", text)\n",
    "    text = re.sub(r\"doesn't\", \"does not\", text)\n",
    "    text = re.sub(r\"i'd\", \"I would\", text)\n",
    "    text = re.sub(r\"didn't\", \"did not\", text)\n",
    "    text = re.sub(r\"ain't\", \"am not\", text)\n",
    "    text = re.sub(r\"you'll\", \"you will\", text)\n",
    "    text = re.sub(r\"I've\", \"I have\", text)\n",
    "    text = re.sub(r\"Don't\", \"do not\", text)\n",
    "    text = re.sub(r\"I'll\", \"I will\", text)\n",
    "    text = re.sub(r\"I'd\", \"I would\", text)\n",
    "    text = re.sub(r\"Let's\", \"Let us\", text)\n",
    "    text = re.sub(r\"you'd\", \"You would\", text)\n",
    "    text = re.sub(r\"It's\", \"It is\", text)\n",
    "    text = re.sub(r\"Ain't\", \"am not\", text)\n",
    "    text = re.sub(r\"Haven't\", \"Have not\", text)\n",
    "    text = re.sub(r\"Could've\", \"Could have\", text)\n",
    "    text = re.sub(r\"youve\", \"you have\", text)  \n",
    "    text = re.sub(r\"donå«t\", \"do not\", text) \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_text = load_file(\"../datas/raw/wikidatas_animals.json\")\n",
    "plant_text = load_file(\"../datas/raw/0wikidatas_plants.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2419/2419 [07:05<00:00,  5.68it/s]\n",
      "100%|██████████| 14510/14510 [59:44<00:00,  4.05it/s]  \n"
     ]
    }
   ],
   "source": [
    "# 词标记化（转化为LSTM+CRF结构输入格式）\n",
    "animal_text_word_token = copy.deepcopy(animal_text)\n",
    "plant_text_word_token = copy.deepcopy(plant_text)\n",
    "sb_stemmer = SnowballStemmer('english')\n",
    "\n",
    "for text_dict in [animal_text_word_token, plant_text_word_token]:\n",
    "    for i in tqdm(text_dict.keys()):\n",
    "        text_container = []\n",
    "        full_doc = file_preprocess(text_dict[i])\n",
    "        doc_split_by_sent = sent_tokenize(full_doc)\n",
    "        for sent in doc_split_by_sent:\n",
    "            word_in_sent = word_tokenize(sent)\n",
    "            wd = [sb_stemmer.stem(word) for word in word_in_sent]\n",
    "            tg = pos_tag(word_in_sent)\n",
    "            text_container.append((wd, tg))\n",
    "        text_dict[i] = text_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二进制形式保存\n",
    "with open('../datas/animal_text_word_token.pkl', 'wb') as bfile:\n",
    "    pickle.dump(animal_text_word_token, bfile, protocol=4)\n",
    "\n",
    "with open('../datas/plant_text_word_token.pkl', 'wb') as bfile:\n",
    "    pickle.dump(plant_text_word_token, bfile, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2419/2419 [08:59<00:00,  4.48it/s]\n",
      "100%|██████████| 14510/14510 [1:04:05<00:00,  3.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# 词标记化（转化为BERT结构输入格式）\n",
    "animal_text_bert_token = copy.deepcopy(animal_text)\n",
    "plant_text_bert_token = copy.deepcopy(plant_text)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "for text_dict in [animal_text_bert_token, plant_text_bert_token]:\n",
    "    for i in tqdm(text_dict.keys()):\n",
    "        text_container = []\n",
    "        full_doc = file_preprocess(text_dict[i])\n",
    "        doc_split_by_sent = sent_tokenize(full_doc)\n",
    "        for sent in doc_split_by_sent:\n",
    "            bert_tokens = tokenizer.tokenize(\"[CLS] \" + sent + \" [SEP]\")\n",
    "            tg = pos_tag(word_tokenize(sent))\n",
    "            text_container.append((bert_tokens, tg))\n",
    "        text_dict[i] = text_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二进制形式保存\n",
    "with open('../datas/token/animal_text_bert_token.pkl', 'wb') as bfile:\n",
    "    pickle.dump(animal_text_bert_token, bfile, protocol=4)\n",
    "\n",
    "with open('../datas/token/plant_text_bert_token.pkl', 'wb') as bfile:\n",
    "    pickle.dump(plant_text_bert_token, bfile, protocol=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
